{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM2-650M LoRA å¾®è°ƒä¸å—œçƒ­æ€§æ”¹é€ å·¥ä½œæµ\n",
    "\n",
    "æœ¬ Notebook å®ç°äº†åŸºäº LoRA (Low-Rank Adaptation) å¯¹ ESM2-650M æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒçš„å®Œæ•´æµç¨‹ã€‚\n",
    "\n",
    "**æ ¸å¿ƒç›®æ ‡ï¼š**\n",
    "1. åˆ©ç”¨å—œçƒ­è›‹ç™½åºåˆ—å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶ä¹ å¾—â€œè€çƒ­é£æ ¼â€ã€‚\n",
    "2. å¯¹æ¯”å¾®è°ƒå‰åæ¨¡å‹å¯¹ç‰¹å®šçªå˜çš„æ‰“åˆ†ï¼ˆLLRï¼‰ï¼Œç­›é€‰æ½œåœ¨çš„å¢ç¨³çªå˜ã€‚\n",
    "\n",
    "**ç¡¬ä»¶è¯´æ˜ï¼š**\n",
    "- ä»£ç è‡ªåŠ¨é€‚é… CUDA (Linux/Windows), MPS (Mac), CPUã€‚\n",
    "- å»ºè®®æ˜¾å­˜ > 12GBã€‚å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œè¯·é™ä½ `BATCH_SIZE` æˆ–ä½¿ç”¨ Gradient Accumulationã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c21f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/plaxy\n"
     ]
    }
   ],
   "source": [
    "%cd /plaxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. å…¨å±€å‚æ•°è®¾ç½® (User Configuration)\n",
    "**è¯·åœ¨æ­¤å¤„ä¿®æ”¹æ‰€æœ‰å…³é”®å‚æ•°ã€‚** è¿™æ˜¯æ•´ä¸ªè„šæœ¬çš„æ§åˆ¶å°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ===========================\n",
    "# 1. [æœ€ä¼˜å…ˆ] è®¾ç½® HuggingFace é•œåƒ\n",
    "# ===========================\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# ===========================\n",
    "# 2. é¡¹ç›®é…ç½®å‚æ•° (Excel æ•°æ®æºæ¨¡å¼)\n",
    "# ===========================\n",
    "\n",
    "# æ¨¡å‹é€‰æ‹©\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "# --- [NEW] æ•°æ®æ–‡ä»¶è®¾ç½® ---\n",
    "# Excel æ–‡ä»¶è·¯å¾„\n",
    "METADATA_FILE = \"uniprot_ph.xlsx\" \n",
    "\n",
    "# Excel åˆ—åæ˜ å°„ (è¯·æ ¹æ®ä½ çš„ Excel è¡¨å¤´ä¿®æ”¹)\n",
    "COL_ENTRY = \"Entry\"          # Uniprot ID åˆ—\n",
    "COL_SEQUENCE = \"Sequence\"    # åºåˆ—å†…å®¹åˆ—\n",
    "COL_MIN_PH = \"min_ph\"        # æœ€å° pH åˆ—\n",
    "COL_MAX_PH = \"max_ph\"        # æœ€å¤§ pH åˆ—\n",
    "SHEET_NAME = 0               # Sheetç´¢å¼• (0) æˆ–åç§°\n",
    "\n",
    "# --- [NEW] ç­›é€‰å‚æ•° ---\n",
    "# é€»è¾‘ï¼šä¿ç•™ (min_ph <= FILTER_MIN_PH) æˆ– (max_ph >= FILTER_MAX_PH) çš„æ•°æ®\n",
    "# å¦‚æœè®¾ä¸º Noneï¼Œåˆ™å¿½ç•¥è¯¥æ¡ä»¶\n",
    "FILTER_MIN_PH = 5.0\n",
    "FILTER_MAX_PH = None\n",
    "\n",
    "# è¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = \"./esm2_lora_ph5.0_less\"\n",
    "\n",
    "# å…¨å±€éšæœºç§å­\n",
    "SEED = 42\n",
    "\n",
    "# ===========================\n",
    "# 3. è®­ç»ƒè¶…å‚æ•°\n",
    "# ===========================\n",
    "LORA_RANK = 8                # LoRA çš„ç§©ï¼Œæ•°æ®é‡å°‘å»ºè®® 8 æˆ– 16\n",
    "LORA_ALPHA = 16              # ç¼©æ”¾ç³»æ•°ï¼Œé€šå¸¸ä¸º Rank çš„ 1-2 å€\n",
    "LORA_DROPOUT = 0.05\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUMULATION = 16\n",
    "\n",
    "# ===========================\n",
    "# 4. åºåˆ—å¤„ç†\n",
    "# ===========================\n",
    "MIN_SEQ_LENGTH = 30\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "MLM_PROBABILITY = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ç¯å¢ƒåˆå§‹åŒ–ä¸ä¾èµ–æ£€æŸ¥\n",
    "å¯¼å…¥å¿…è¦çš„åº“ï¼Œè®¾ç½®éšæœºç§å­ï¼Œå¹¶è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„è®¡ç®—è®¾å¤‡ (CUDA/MPS/CPU)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ­£åœ¨ä½¿ç”¨è®¡ç®—è®¾å¤‡: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, EsmForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "# æ£€æŸ¥ PEFT æ˜¯å¦å®‰è£…\n",
    "try:\n",
    "    import peft\n",
    "except ImportError:\n",
    "    !pip install peft -q\n",
    "\n",
    "# 1. è®¾ç½®éšæœºç§å­å‡½æ•°\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# 2. è®¾å¤‡æ£€æµ‹\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"ğŸ”¥ æ­£åœ¨ä½¿ç”¨è®¡ç®—è®¾å¤‡: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPUå‹å·: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. æ•°æ®å‡†å¤‡ (Data Preparation)\n",
    "è¯»å– FASTA æ–‡ä»¶å¹¶å°†å…¶è½¬æ¢ä¸º HuggingFace Dataset æ ¼å¼ã€‚\n",
    "*æ³¨ï¼šå¦‚æœæŒ‡å®šè·¯å¾„ä¸‹æ²¡æœ‰ FASTA æ–‡ä»¶ï¼Œè¿™é‡Œä¼šç”Ÿæˆä¸€ä¸ªå‡çš„ FASTA æ–‡ä»¶ç”¨äºæ¼”ç¤ºã€‚*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ æ­£åœ¨è¯»å–æ•°æ®: uniprot_ph.xlsx\n",
      "   æ£€æµ‹åˆ° Excel æ ¼å¼\n",
      "ğŸ“Š åŸå§‹æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± 5462 è¡Œã€‚\n",
      "âœ… ç­›é€‰æ¡ä»¶ A åº”ç”¨: min_ph <= 5.0\n",
      "ğŸ“‰ pH ç­›é€‰ç»“æœ: 5462 -> 716 (ä¿ç•™æ»¡è¶³ä»»ä¸€æ¡ä»¶çš„æ¡ç›®)\n",
      "ğŸ“‹ æœ€ç»ˆç”¨äºè®­ç»ƒçš„æœ‰æ•ˆåºåˆ—æ•°: 716\n",
      "ğŸ§¹ æ­£åœ¨è¿‡æ»¤é•¿åº¦: ä¿ç•™ 30 åˆ° 1024 ä¹‹é—´çš„åºåˆ—...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c055187dddb4b618fdc22839bd7c75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‰ é•¿åº¦è¿‡æ»¤ç»“æœ: 716 -> 668 (å‰”é™¤äº† 48 æ¡)\n",
      "ğŸš† è®­ç»ƒé›†å¤§å°: 601\n",
      "ğŸ§ª éªŒè¯é›†å¤§å°: 67\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# æ£€æŸ¥ pandas å’Œ openpyxl æ˜¯å¦å®‰è£…\n",
    "try:\n",
    "    import pandas\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    print(\"æ­£åœ¨å®‰è£…å¿…è¦çš„åº“ pandas openpyxl ...\")\n",
    "    !pip install pandas openpyxl -q\n",
    "\n",
    "# --- è¾…åŠ©å‡½æ•°ï¼šæ¸…æ´— pH å€¼ ---\n",
    "def clean_ph_value(val):\n",
    "    \"\"\"\n",
    "    å°†è¾“å…¥è½¬æ¢ä¸º floatï¼Œå¤„ç†éæ•°å€¼æƒ…å†µã€‚\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return pd.NA\n",
    "    try:\n",
    "        # å°è¯•ç›´æ¥è½¬æ¢\n",
    "        return float(val)\n",
    "    except (ValueError, TypeError):\n",
    "        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•å¤„ç†å¤æ‚æƒ…å†µï¼ˆå¦‚ '5.5' æˆ–å¸¦ç©ºæ ¼ï¼‰\n",
    "        # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„æ¸…æ´—ï¼Œå¦‚æœæ— æ³•è½¬æ¢åˆ™è¿”å› NA\n",
    "        try:\n",
    "            import re\n",
    "            # æå–ç¬¬ä¸€ä¸ªæµ®ç‚¹æ•°\n",
    "            match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", str(val))\n",
    "            if match:\n",
    "                return float(match.group())\n",
    "            return pd.NA\n",
    "        except:\n",
    "            return pd.NA\n",
    "\n",
    "# --- 1. è¯»å– Excel æ•°æ® ---\n",
    "if not os.path.exists(METADATA_FILE):\n",
    "    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿçš„ Excel ç”¨äºæµ‹è¯•ä»£ç è¿è¡Œ\n",
    "    print(f\"âš ï¸ æœªæ‰¾åˆ° {METADATA_FILE}ï¼Œæ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿ Excel æ•°æ®ä»¥ä¾›æ¼”ç¤º...\")\n",
    "    dummy_data = {\n",
    "        COL_ENTRY: [f\"P{i}\" for i in range(100)],\n",
    "        COL_SEQUENCE: [\n",
    "            \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\",\n",
    "            \"MSIGTGYTPAIYNGSLKQGKPVISLSIGDTVTIDQKADIYAKLDNLVAQGKTFVIIADEAYA\"\n",
    "        ] * 50,\n",
    "        COL_MIN_PH: [0.0, 4.0, 6.0, 2.0, None] * 20,\n",
    "        COL_MAX_PH: [4.0, 6.0, 9.0, 10.0, 8.0] * 20\n",
    "    }\n",
    "    df = pd.DataFrame(dummy_data)\n",
    "    # ä¸´æ—¶ä¿å­˜ä»¥ä¾¿æµç¨‹ç»§ç»­ï¼Œå®é™…ä½¿ç”¨è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®\n",
    "    METADATA_FILE = \"dummy_data.xlsx\" \n",
    "    df.to_excel(METADATA_FILE, index=False)\n",
    "    print(f\"âœ… æ¨¡æ‹Ÿæ•°æ®å·²ä¿å­˜è‡³ {METADATA_FILE}\")\n",
    "else:\n",
    "    print(f\"ğŸ“‚ æ­£åœ¨è¯»å–æ•°æ®: {METADATA_FILE}\")\n",
    "\n",
    "# è¯»å– Excel\n",
    "# è·å–æ–‡ä»¶æ‰©å±•å\n",
    "file_ext = os.path.splitext(METADATA_FILE)[-1].lower()\n",
    "\n",
    "try:\n",
    "    if file_ext in ['.xlsx', '.xls']:\n",
    "        # è¯»å– Excel\n",
    "        print(\"   æ£€æµ‹åˆ° Excel æ ¼å¼\")\n",
    "        df = pd.read_excel(METADATA_FILE, sheet_name=SHEET_NAME)\n",
    "        \n",
    "    elif file_ext == '.tsv':\n",
    "        # è¯»å– TSV (Tab åˆ†éš”)\n",
    "        print(\"   æ£€æµ‹åˆ° TSV æ ¼å¼\")\n",
    "        df = pd.read_csv(METADATA_FILE, sep='\\t')\n",
    "        \n",
    "    elif file_ext == '.csv':\n",
    "        # è¯»å– CSV (é€—å·åˆ†éš”)\n",
    "        print(\"   æ£€æµ‹åˆ° CSV æ ¼å¼\")\n",
    "        df = pd.read_csv(METADATA_FILE)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚ä»…æ”¯æŒ .xlsx, .xls, .tsv, .csv\")\n",
    "\n",
    "    print(f\"ğŸ“Š åŸå§‹æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} è¡Œã€‚\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "# --- 2. åº”ç”¨ç­›é€‰é€»è¾‘ (User Defined Logic) ---\n",
    "\n",
    "# é¢„å¤„ç†ï¼šç”Ÿæˆä¸´æ—¶çš„æ•°å€¼å‹ pH åˆ—\n",
    "if COL_MIN_PH and COL_MIN_PH in df.columns:\n",
    "    df['temp_min_ph'] = df[COL_MIN_PH].apply(clean_ph_value)\n",
    "else:\n",
    "    df['temp_min_ph'] = pd.NA\n",
    "\n",
    "if COL_MAX_PH and COL_MAX_PH in df.columns:\n",
    "    df['temp_max_ph'] = df[COL_MAX_PH].apply(clean_ph_value)\n",
    "else:\n",
    "    df['temp_max_ph'] = pd.NA\n",
    "\n",
    "# åˆå§‹åŒ–ç­›é€‰æ©ç  (Mask)\n",
    "if FILTER_MIN_PH is None and FILTER_MAX_PH is None:\n",
    "    mask = pd.Series([True] * len(df), index=df.index)\n",
    "    print(\"ğŸ’¡ æç¤º: æœªè®¾ç½®ä»»ä½• pH ç­›é€‰æ¡ä»¶ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®ã€‚\")\n",
    "else:\n",
    "    mask = pd.Series([False] * len(df), index=df.index)\n",
    "    filter_applied = False\n",
    "\n",
    "    # æ¡ä»¶ A: min_ph <= FILTER_MIN_PH\n",
    "    if FILTER_MIN_PH is not None and 'temp_min_ph' in df.columns:\n",
    "        condition_min = (df['temp_min_ph'].notna()) & (df['temp_min_ph'] <= FILTER_MIN_PH)\n",
    "        mask = mask | condition_min  # é€»è¾‘ OR\n",
    "        print(f\"âœ… ç­›é€‰æ¡ä»¶ A åº”ç”¨: {COL_MIN_PH} <= {FILTER_MIN_PH}\")\n",
    "        filter_applied = True\n",
    "\n",
    "    # æ¡ä»¶ B: max_ph >= FILTER_MAX_PH\n",
    "    if FILTER_MAX_PH is not None and 'temp_max_ph' in df.columns:\n",
    "        condition_max = (df['temp_max_ph'].notna()) & (df['temp_max_ph'] >= FILTER_MAX_PH)\n",
    "        mask = mask | condition_max  # é€»è¾‘ OR\n",
    "        print(f\"âœ… ç­›é€‰æ¡ä»¶ B åº”ç”¨: {COL_MAX_PH} >= {FILTER_MAX_PH}\")\n",
    "        filter_applied = True\n",
    "\n",
    "    # åº”ç”¨ç­›é€‰\n",
    "    original_count = len(df)\n",
    "    df = df[mask]\n",
    "    print(f\"ğŸ“‰ pH ç­›é€‰ç»“æœ: {original_count} -> {len(df)} (ä¿ç•™æ»¡è¶³ä»»ä¸€æ¡ä»¶çš„æ¡ç›®)\")\n",
    "\n",
    "# --- 3. æ•°æ®æ¸…æ´— (å»é™¤æ— æ•ˆåºåˆ—) ---\n",
    "# ç¡®ä¿åºåˆ—åˆ—å­˜åœ¨ä¸”ä¸ä¸ºç©º\n",
    "if COL_SEQUENCE not in df.columns:\n",
    "    raise ValueError(f\"é”™è¯¯: Excel ä¸­æ‰¾ä¸åˆ°åºåˆ—åˆ— '{COL_SEQUENCE}'\")\n",
    "\n",
    "# å»é™¤ NaN åºåˆ—\n",
    "df = df.dropna(subset=[COL_SEQUENCE])\n",
    "# å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "df[COL_SEQUENCE] = df[COL_SEQUENCE].astype(str)\n",
    "# ç®€å•çš„å»é‡ï¼ˆå¯é€‰ï¼Œè§†éœ€æ±‚è€Œå®šï¼Œè¿™é‡Œå»ºè®®ä¿ç•™å»é‡ä»¥é˜²è¿‡æ‹Ÿåˆï¼‰\n",
    "# df = df.drop_duplicates(subset=[COL_SEQUENCE])\n",
    "\n",
    "# æå–åºåˆ—åˆ—è¡¨\n",
    "raw_sequences = df[COL_SEQUENCE].tolist()\n",
    "print(f\"ğŸ“‹ æœ€ç»ˆç”¨äºè®­ç»ƒçš„æœ‰æ•ˆåºåˆ—æ•°: {len(raw_sequences)}\")\n",
    "\n",
    "# è½¬æ¢ä¸º Dataset å¯¹è±¡\n",
    "dataset = Dataset.from_dict({\"text\": raw_sequences})\n",
    "\n",
    "# ===========================\n",
    "# [ä¿æŒåŸæœ‰] é•¿åº¦è¿‡æ»¤ (Data Cleaning)\n",
    "# ===========================\n",
    "MIN_LEN = MIN_SEQ_LENGTH\n",
    "MAX_LEN = MAX_SEQ_LENGTH\n",
    "\n",
    "print(f\"ğŸ§¹ æ­£åœ¨è¿‡æ»¤é•¿åº¦: ä¿ç•™ {MIN_LEN} åˆ° {MAX_LEN} ä¹‹é—´çš„åºåˆ—...\")\n",
    "original_count = len(dataset)\n",
    "dataset = dataset.filter(lambda x: MIN_LEN <= len(x[\"text\"]) <= MAX_LEN)\n",
    "filtered_count = len(dataset)\n",
    "print(f\"ğŸ“‰ é•¿åº¦è¿‡æ»¤ç»“æœ: {original_count} -> {filtered_count} (å‰”é™¤äº† {original_count - filtered_count} æ¡)\")\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (90/10)\n",
    "if len(dataset) > 0:\n",
    "    dataset_split = dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_dataset = dataset_split[\"train\"]\n",
    "    eval_dataset = dataset_split[\"test\"]\n",
    "    print(f\"ğŸš† è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "    print(f\"ğŸ§ª éªŒè¯é›†å¤§å°: {len(eval_dataset)}\")\n",
    "else:\n",
    "    raise ValueError(\"âŒ é”™è¯¯ï¼šç»è¿‡ç­›é€‰åæ²¡æœ‰å‰©ä½™æ•°æ®ï¼è¯·æ£€æŸ¥ Excel æ•°æ®æˆ–æ”¾å®½ç­›é€‰æ¡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db727b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡é•¿åº¦: 417.4\n",
      "90% çš„è›‹ç™½é•¿åº¦å°äº: 676\n",
      "95% çš„è›‹ç™½é•¿åº¦å°äº: 785\n",
      "99% çš„è›‹ç™½é•¿åº¦å°äº: 924\n"
     ]
    }
   ],
   "source": [
    "# åºåˆ—é•¿åº¦åˆ†å¸ƒç»Ÿè®¡\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# æå–æ‰€æœ‰åºåˆ—é•¿åº¦\n",
    "seq_lens = [len(x[\"text\"]) for x in dataset] # å‡è®¾ dataset æ˜¯åˆšæ‰ç”Ÿæˆçš„ Dataset å¯¹è±¡\n",
    "\n",
    "# è®¡ç®—åˆ†ä½æ•°\n",
    "p90 = np.percentile(seq_lens, 90)\n",
    "p95 = np.percentile(seq_lens, 95)\n",
    "p99 = np.percentile(seq_lens, 99)\n",
    "\n",
    "print(f\"å¹³å‡é•¿åº¦: {np.mean(seq_lens):.1f}\")\n",
    "print(f\"90% çš„è›‹ç™½é•¿åº¦å°äº: {p90:.0f}\")\n",
    "print(f\"95% çš„è›‹ç™½é•¿åº¦å°äº: {p95:.0f}\")\n",
    "print(f\"99% çš„è›‹ç™½é•¿åº¦å°äº: {p99:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. åºåˆ—åˆ†è¯ (Tokenization)\n",
    "ä½¿ç”¨ ESM2 çš„ tokenizer å¯¹åºåˆ—è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬éœ€è¦è®¾ç½® `truncation` æ¥å¤„ç†è¿‡é•¿çš„åºåˆ—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨è¿›è¡Œåˆ†è¯å¤„ç†...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb17dfe303b4abbaf7f84d2f6e0880c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56e8e7a3048498aaf6d016aeff4eddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ†è¯å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=False,\n",
    "        # padding=\"max_length\",  # --- IGNORE --- \n",
    "        truncation=True, \n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "print(\"â³ æ­£åœ¨è¿›è¡Œåˆ†è¯å¤„ç†...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "print(\"âœ… åˆ†è¯å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. æ¨¡å‹åŠ è½½ä¸ LoRA é…ç½® (Model & LoRA Setup)\n",
    "åŠ è½½ ESM2-650M æ¨¡å‹ï¼Œå¹¶åº”ç”¨ LoRA é€‚é…å™¨ã€‚\n",
    "**æ³¨æ„**ï¼šè¿™é‡Œæˆ‘ä»¬åªå¾®è°ƒ `query`, `key`, `value` å±‚ï¼Œè¿™æ˜¯æœ€èŠ‚çœèµ„æºä¸”æœ‰æ•ˆçš„æ–¹å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,103,040 || all params: 657,146,294 || trainable%: 0.9287\n",
      "ğŸš€ æ¨¡å‹å·²åŠ è½½è‡³æ˜¾å­˜å¹¶æŒ‚è½½ LoRAã€‚\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ Masked Language Model å¤´éƒ¨çš„ ESM2\n",
    "model = EsmForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# é…ç½® LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION, # æˆ–ä½¿ç”¨ generic task configuration\n",
    "    inference_mode=False,\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    # é’ˆå¯¹ ESM2 æ¶æ„ï¼Œå¾®è°ƒæ³¨æ„åŠ›æœºåˆ¶çš„ Q/K/V\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"], \n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# å°† LoRA æŒ‚è½½åˆ°æ¨¡å‹ä¸Š\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# æ‰“å°å¯è®­ç»ƒå‚æ•°é‡ï¼Œç¡®è®¤å¾®è°ƒè§„æ¨¡\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.to(device)\n",
    "print(\"ğŸš€ æ¨¡å‹å·²åŠ è½½è‡³æ˜¾å­˜å¹¶æŒ‚è½½ LoRAã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. å¼€å§‹è®­ç»ƒ (Training)\n",
    "ä½¿ç”¨ HuggingFace `Trainer` è¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨ `DataCollatorForLanguageModeling` è‡ªåŠ¨è¿›è¡Œéšæœº Masking (MLM ä»»åŠ¡)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ å¼€å§‹è®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghaonan/miniforge3/envs/oper/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 34:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.442300</td>\n",
       "      <td>1.392439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.479800</td>\n",
       "      <td>1.386320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.432400</td>\n",
       "      <td>1.309498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghaonan/miniforge3/envs/oper/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/zhanghaonan/miniforge3/envs/oper/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ ./esm2_lora_ph5.0_less/final_model\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# è¡¥å…¨ç¼ºå¤±çš„ Data Collator å®šä¹‰\n",
    "# ===========================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=True, \n",
    "    mlm_probability=MLM_PROBABILITY,\n",
    "    pad_to_multiple_of=8  # <--- âš¡ï¸ æ–°å¢ï¼šè‡ªåŠ¨å¡«å……åˆ°å½“å‰ batch æœ€é•¿åºåˆ—ï¼Œä¸”å¯¹é½ä¸º 8 çš„å€æ•°ï¼ˆåŠ é€Ÿè®¡ç®—ï¼‰\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# è®­ç»ƒå‚æ•°é…ç½® (å·²ä¿®å¤å‚æ•°åå’Œ MPS é—®é¢˜)\n",
    "# ===========================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    # === âš¡ï¸ æ–°å¢ç¥å™¨ âš¡ï¸ ===\n",
    "    group_by_length=True,              # è‡ªåŠ¨æŠŠé•¿åº¦ç›¸ä¼¼çš„è›‹ç™½å‡‘ä¸€ç»„ï¼Œæå¤§å‡å°‘ Padding æµªè´¹\n",
    "    \n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",        # ä½¿ç”¨æ–°ç‰ˆå‚æ•°å eval_strategy\n",
    "    load_best_model_at_end=True,  # è®­ç»ƒç»“æŸåŠ è½½æœ€å¥½çš„æ¨¡å‹\n",
    "    fp16=torch.cuda.is_available(), # å¦‚æœæ˜¯ CUDA å¼€å¯æ··åˆç²¾åº¦\n",
    "    # use_mps_device=(device.type == 'mps'), # å·²ç§»é™¤ï¼Œæ–°ç‰ˆè‡ªåŠ¨å¤„ç†\n",
    "    seed=SEED,\n",
    "    report_to=\"none\" # ä¸ä¸Šä¼  wandb\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# åˆå§‹åŒ– Trainer\n",
    "# ===========================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator, # è¿™é‡Œå°±ä¸ä¼šæŠ¥é”™äº†\n",
    ")\n",
    "\n",
    "print(\"ğŸ å¼€å§‹è®­ç»ƒ...\")\n",
    "trainer.train()\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆçš„ LoRA æƒé‡\n",
    "save_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "model.save_pretrained(save_path)\n",
    "print(f\"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. åº”ç”¨æ¼”ç¤ºï¼šå¯¹æ¯”æ‰“åˆ† (In silico Screening)\n",
    "è¿™æ˜¯æœ¬æµç¨‹çš„æ ¸å¿ƒï¼š**è®¡ç®—çªå˜çš„ç›¸å¯¹ä¼¼ç„¶åº¦ (LLR Score)**ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†å¯¹æ¯”åŒä¸€çªå˜åœ¨ **åŸå§‹æ¨¡å‹ (Base)** å’Œ **å¾®è°ƒæ¨¡å‹ (Fine-tuned)** ä¸‹çš„æ¦‚ç‡å·®å€¼ã€‚\n",
    "**æŠ€æœ¯ç‚¹**ï¼šä½¿ç”¨ `model.disable_adapter()` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸å¸è½½/é‡è½½æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œç¬é—´åˆ‡æ¢å›åŸå§‹ ESM2 è¿›è¡Œæ¨ç†ï¼Œæå¤§èŠ‚çœæ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” çªå˜åˆ†æ: D36A\n",
      "   ä½ç‚¹: 36 | é‡ç”Ÿå‹: D -> çªå˜å‹: A\n",
      "   ---------------------------\n",
      "   åŸå§‹æ¨¡å‹å¾—åˆ† (LogProb): -3.5073\n",
      "   å¾®è°ƒæ¨¡å‹å¾—åˆ† (LogProb): -3.4917\n",
      "   >>> LLR æå‡ (Score): 0.0156\n",
      "ğŸ‰ æ¨è: D36A å¯èƒ½æ˜¯å¢ç¨³çªå˜ï¼\n"
     ]
    }
   ],
   "source": [
    "def calculate_mutation_score(seq, position_1based, mutant_aa, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç‰¹å®šä½ç‚¹çªå˜çš„ LLR åˆ†æ•°ã€‚\n",
    "    å‚æ•°:\n",
    "        position_1based: ç”Ÿç‰©å­¦ä½ç½®ï¼Œä» 1 å¼€å§‹è®¡æ•° (ä¾‹å¦‚ç¬¬ 1 ä¸ªæ°¨åŸºé…¸è¾“å…¥ 1)\n",
    "    \"\"\"\n",
    "    # 1. è¾¹ç•Œæ£€æŸ¥ä¸åŸºç¡€ä¿¡æ¯è·å–\n",
    "    seq_len = len(seq)\n",
    "    if position_1based < 1 or position_1based > seq_len:\n",
    "        raise ValueError(f\"ä½ç½® {position_1based} è¶…å‡ºåºåˆ—èŒƒå›´ (1-{seq_len})\")\n",
    "    \n",
    "    # è·å–é‡ç”Ÿå‹æ°¨åŸºé…¸ (Python string ç´¢å¼•éœ€è¦ -1)\n",
    "    wt_aa = seq[position_1based - 1] \n",
    "    \n",
    "    # 2. å‡†å¤‡æ¨¡å‹è¾“å…¥\n",
    "    model.eval()\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH).to(device)\n",
    "    \n",
    "    # è·å–çªå˜æ°¨åŸºé…¸çš„ Token ID\n",
    "    mut_token_id = tokenizer.convert_tokens_to_ids(mutant_aa)\n",
    "    \n",
    "    # 3. ç¡®å®š Tensor ä¸­çš„ç´¢å¼•ä½ç½®\n",
    "    # ESM2 çš„ input_ids æ ¼å¼ä¸º: [CLS, AA1, AA2, ..., AAn, EOS]\n",
    "    # ç´¢å¼• 0 æ˜¯ CLS\n",
    "    # ç´¢å¼• 1 æ˜¯ AA1 (å¯¹åº” position_1based=1)\n",
    "    # æ‰€ä»¥ Tensor ç´¢å¼• = position_1based\n",
    "    tensor_index = position_1based \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- å¾®è°ƒæ¨¡å‹ (Fine-tuned) ---\n",
    "        outputs_tuned = model(**inputs)\n",
    "        log_probs_tuned = torch.nn.functional.log_softmax(outputs_tuned.logits, dim=-1)\n",
    "        score_tuned = log_probs_tuned[0, tensor_index, mut_token_id].item()\n",
    "        \n",
    "        # --- åŸå§‹æ¨¡å‹ (Base) ---\n",
    "        with model.disable_adapter():\n",
    "            outputs_base = model(**inputs)\n",
    "            log_probs_base = torch.nn.functional.log_softmax(outputs_base.logits, dim=-1)\n",
    "            score_base = log_probs_base[0, tensor_index, mut_token_id].item()\n",
    "            \n",
    "    llr_score = score_tuned - score_base\n",
    "    return wt_aa, score_tuned, score_base, llr_score\n",
    "\n",
    "# --- æ¼”ç¤º ---\n",
    "# å‡è®¾è¿™æ˜¯ä½ çš„å¾…æ”¹é€ è›‹ç™½\n",
    "target_protein = \"VDNKFNKEQQNAFYEILHLPNLNEEQRNAFIQSLKDDPSQSANLLAEAKKLNDAQAPK\"\n",
    "\n",
    "# ã€ç”¨æˆ·ä¿®æ”¹åŒºã€‘ è®¾ç½®ç¬¦åˆç”Ÿç‰©å­¦ç›´è§‰çš„å‚æ•°\n",
    "test_pos = 36      # ç›´æ¥å†™ç¬¬ 26 ä½\n",
    "test_mut = \"A\"     # çªå˜æˆ R\n",
    "\n",
    "# è®¡ç®—\n",
    "wt, st, sb, llr = calculate_mutation_score(target_protein, test_pos, test_mut, model, tokenizer, device)\n",
    "\n",
    "# ç”Ÿæˆæ ‡å‡†çªå˜åç§° (å¦‚ A26R)\n",
    "mutation_name = f\"{wt}{test_pos}{test_mut}\"\n",
    "\n",
    "print(f\"ğŸ” çªå˜åˆ†æ: {mutation_name}\")\n",
    "print(f\"   ä½ç‚¹: {test_pos} | é‡ç”Ÿå‹: {wt} -> çªå˜å‹: {test_mut}\")\n",
    "print(f\"   ---------------------------\")\n",
    "print(f\"   åŸå§‹æ¨¡å‹å¾—åˆ† (LogProb): {sb:.4f}\")\n",
    "print(f\"   å¾®è°ƒæ¨¡å‹å¾—åˆ† (LogProb): {st:.4f}\")\n",
    "print(f\"   >>> LLR æå‡ (Score): {llr:.4f}\")\n",
    "\n",
    "if llr > 0:\n",
    "    print(f\"ğŸ‰ æ¨è: {mutation_name} å¯èƒ½æ˜¯å¢ç¨³çªå˜ï¼\")\n",
    "else:\n",
    "    print(f\"â„ï¸ ä¸æ¨è: æ¨¡å‹å¯¹ {mutation_name} ååº”æ¶ˆæã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. æ‰¹é‡å•ç‚¹é¥±å’Œçªå˜æ‰«æ (In silico DMS)\n",
    "å¯¹æŒ‡å®šåŒºåŸŸè¿›è¡Œå…¨æ°¨åŸºé…¸æ‰«æï¼Œå¯»æ‰¾ LLR æœ€é«˜çš„çªå˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ­£åœ¨æ‰«æåŒºåŸŸ 1 - 10 (1-based)...\n",
      "\\nğŸ† Top 10 æ¨èçªå˜ (Region 1-10):\n",
      "Mutation   | Score     \n",
      "-------------------------\n",
      "V1W        | 0.6752\n",
      "V1F        | 0.6234\n",
      "V1L        | 0.6156\n",
      "V1Q        | 0.6039\n",
      "V1P        | 0.5775\n",
      "V1Y        | 0.5765\n",
      "V1A        | 0.5756\n",
      "V1R        | 0.5599\n",
      "V1S        | 0.5551\n",
      "V1K        | 0.5546\n"
     ]
    }
   ],
   "source": [
    "# æ ‡å‡†æ°¨åŸºé…¸åˆ—è¡¨\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "# ã€ç”¨æˆ·ä¿®æ”¹åŒºã€‘ è®¾ç½®æ‰«æèŒƒå›´ (1-based, åŒ…å«ä¸¤ç«¯)\n",
    "scan_start = 1   # ä»ç¬¬ 5 ä¸ªæ°¨åŸºé…¸å¼€å§‹\n",
    "scan_end = 10    # åˆ°ç¬¬ 10 ä¸ªæ°¨åŸºé…¸ç»“æŸ\n",
    "\n",
    "results = []\n",
    "print(f\"ğŸ”„ æ­£åœ¨æ‰«æåŒºåŸŸ {scan_start} - {scan_end} (1-based)...\")\n",
    "\n",
    "# å¾ªç¯èŒƒå›´ï¼šrange(start, end + 1) ä»¥åŒ…å«ç»“æŸä½ç‚¹\n",
    "for pos in range(scan_start, scan_end + 1):\n",
    "    # ç®€å•çš„è¾¹ç•Œä¿æŠ¤\n",
    "    if pos > len(target_protein): break\n",
    "    \n",
    "    # éå† 20 ç§æ°¨åŸºé…¸\n",
    "    for aa in amino_acids:\n",
    "        # è°ƒç”¨å‡½æ•° (wt_aa åœ¨å‡½æ•°å†…éƒ¨è·å–)\n",
    "        wt, _, _, score = calculate_mutation_score(target_protein, pos, aa, model, tokenizer, device)\n",
    "        \n",
    "        if aa == wt: \n",
    "            continue # è·³è¿‡åŒä¹‰çªå˜\n",
    "            \n",
    "        if score > 0.0: # åªè®°å½•åˆ†æ•°å¤§äº 0 çš„ (èŠ‚çœæ˜¾ç¤ºç©ºé—´ï¼Œå¯æŒ‰éœ€è°ƒæ•´)\n",
    "            mutation_code = f\"{wt}{pos}{aa}\"\n",
    "            results.append({\n",
    "                \"Mutation\": mutation_code,\n",
    "                \"Pos\": pos,\n",
    "                \"WT\": wt,\n",
    "                \"Mut\": aa,\n",
    "                \"LLR_Score\": score\n",
    "            })\n",
    "\n",
    "# æŒ‰ç…§åˆ†æ•°ä»é«˜åˆ°ä½æ’åº\n",
    "results.sort(key=lambda x: x[\"LLR_Score\"], reverse=True)\n",
    "\n",
    "print(f\"\\\\nğŸ† Top 10 æ¨èçªå˜ (Region {scan_start}-{scan_end}):\")\n",
    "print(f\"{'Mutation':<10} | {'Score':<10}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for res in results[:10]:\n",
    "    print(f\"{res['Mutation']:<10} | {res['LLR_Score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
